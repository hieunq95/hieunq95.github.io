---
title: "Point Cloud Compression with Bits-back Coding"
collection: publications
category: manuscripts
permalink: /publication/2024-10-paper-1
excerpt: "Storing and sending large 3D point clouds takes significant digital space, demanding efficient compression methods that preserve all details. We introduce a new AI-powered approach that effectively compresses this data while significantly reducing the size of the decompression instructions needed, unlike many other methods. This makes our technique highly practical and allows it to achieve better compression results than standard tools like Google's Draco."
date: 2024-10-09
venue: 'arXiv preprint'
# slidesurl: 'http://academicpages.github.io/files/slides1.pdf'
paperurl: 'https://arxiv.org/abs/2410.18115'
# bibtexurl: 'http://academicpages.github.io/files/bibtex1.bib'
citation: 'N. Q. Hieu, M. Nguyen, D. T. Hoang, D. N. Nguyen, E. Dutkiewicz, "Point Cloud Compression with Bits-back Coding", 2024.'
---

**Abstract:** This paper introduces a novel lossless compression method for compressing geometric attributes of point cloud data with bits-back coding. Our method specializes in using a deep learning-based probabilistic model to estimate the Shannon's entropy of the point cloud information, i.e., geometric attributes of the 3D floating points. Once the entropy of the point cloud dataset is estimated with a convolutional variational autoencoder (CVAE), we use the learned CVAE model to compress the geometric attributes of the point clouds with the bits-back coding technique. The novelty of our method with bits-back coding specializes in utilizing the learned latent variable model of the CVAE to compress the point cloud data. By using bits-back coding, we can capture the potential correlation between the data points, such as similar spatial features like shapes and scattering regions, into the lower-dimensional latent space to further reduce the compression ratio. The main insight of our method is that we can achieve a competitive compression ratio as conventional deep learning-based approaches, while significantly reducing the overhead cost of storage and/or communicating the compression codec, making our approach more applicable in practical scenarios. Throughout comprehensive evaluations, we found that the cost for the overhead is significantly small, compared to the reduction of the compression ratio when compressing large point cloud datasets. Experiment results show that our proposed approach can achieve a compression ratio of 1.56 bit-per-point on average, which is significantly lower than the baseline approach such as Google's Draco with a compression ratio of 1.83 bit-per-point.
